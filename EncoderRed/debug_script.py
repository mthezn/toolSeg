import torch
import torch.nn as nn
from fvcore.nn import FlopCountAnalysis
import torch

def debug_model_parameters(model, top_n=20):
    """
    Trova dove sono i parametri nel modello.
    """
    print("=" * 80)
    print("üîç DEBUG PARAMETRI MODELLO")
    print("=" * 80)

    # Conta totale
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\nüìä TOTALE:")
    print(f"  ‚Ä¢ Parametri totali:     {total_params:>15,} ({total_params / 1e6:.2f}M)")
    print(f"  ‚Ä¢ Parametri trainable:  {trainable_params:>15,} ({trainable_params / 1e6:.2f}M)")

    # Analizza ogni modulo
    print(f"\nüìã TOP {top_n} MODULI PER PARAMETRI:")
    print("-" * 80)
    print(f"{'Nome Modulo':<50} {'Parametri':>15} {'%':>8}")
    print("-" * 80)

    module_params = []
    for name, module in model.named_modules():
        # Conta solo i parametri diretti del modulo (non dei figli)
        params = sum(p.numel() for p in module.parameters(recurse=False))
        if params > 0:
            module_params.append({
                'name': name if name else 'root',
                'params': params,
                'percentage': (params / total_params) * 100
            })

    # Ordina per numero di parametri
    module_params.sort(key=lambda x: x['params'], reverse=True)

    for item in module_params[:top_n]:
        print(f"{item['name']:<50} {item['params']:>15,} {item['percentage']:>7.2f}%")

    # Analizza per tipo di layer
    print(f"\nüì¶ PARAMETRI PER TIPO DI LAYER:")
    print("-" * 80)

    layer_types = {}
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # Solo foglie
            layer_type = module.__class__.__name__
            params = sum(p.numel() for p in module.parameters())

            if layer_type not in layer_types:
                layer_types[layer_type] = {'count': 0, 'params': 0}
            layer_types[layer_type]['count'] += 1
            layer_types[layer_type]['params'] += params

    sorted_types = sorted(layer_types.items(), key=lambda x: x[1]['params'], reverse=True)

    print(f"{'Tipo Layer':<30} {'Count':>8} {'Parametri':>15} {'%':>8}")
    print("-" * 80)
    for layer_type, info in sorted_types:
        percentage = (info['params'] / total_params) * 100
        print(f"{layer_type:<30} {info['count']:>8} {info['params']:>15,} {percentage:>7.2f}%")

    # Analizza per stage
    print(f"\nüéØ PARAMETRI PER STAGE:")
    print("-" * 80)

    stages = ['stem', 'patch1', 'stage1', 'patch2', 'stage2',
              'patch3', 'stage3', 'patch4', 'stage4', 'conv', 'upsample']

    for stage_name in stages:
        if hasattr(model, stage_name):
            stage = getattr(model, stage_name)
            stage_params = sum(p.numel() for p in stage.parameters())
            percentage = (stage_params / total_params) * 100
            print(f"{stage_name:<20} {stage_params:>15,} ({stage_params / 1e6:>6.2f}M) {percentage:>7.2f}%")

    print("\n" + "=" * 80)
    import torch
    from thop import profile
    # model: la tua istanza nn.Module

    input_image = torch.randn(1, 3, 1024, 1024).to(next(model.parameters()).device)
    """ 
    batched_input = [{
        "image": input_image.squeeze(0),  # rimuove dimensione batch extra se serve
        "original_size": (1024, 1024) per edge sam
    }]
    """

    # multimask_output = False per semplificare
    flops = FlopCountAnalysis(model, input_image)

    print("Total FLOPs:", flops.total())

    return total_params


def find_problematic_layers(model, threshold_mb=10):
    """
    Trova layer con troppi parametri (potenziali problemi).
    """
    print(f"\n‚ö†Ô∏è  LAYER SOSPETTI (>{threshold_mb}MB):")
    print("-" * 80)

    suspicious = []
    for name, param in model.named_parameters():
        size_mb = param.numel() * 4 / (1024 ** 2)  # float32 = 4 bytes
        if size_mb > threshold_mb:
            suspicious.append({
                'name': name,
                'shape': list(param.shape),
                'params': param.numel(),
                'size_mb': size_mb
            })

    suspicious.sort(key=lambda x: x['size_mb'], reverse=True)

    if suspicious:
        print(f"{'Nome Parametro':<50} {'Shape':<25} {'Size (MB)':>12}")
        print("-" * 80)
        for item in suspicious[:10]:
            print(f"{item['name']:<50} {str(item['shape']):<25} {item['size_mb']:>11.2f}")
    else:
        print("‚úì Nessun layer sospetto trovato")

    return suspicious


def compare_with_reference():
    """
    Confronta con i valori di riferimento per CMT-Ti.
    """
    print("\nüìö RIFERIMENTO CMT-Ti (Paper Originale):")
    print("-" * 80)

    reference = {
        'Total Params': '9.5M',
        'Input Size': '224x224',
        'FLOPs': '0.4G',
        'Channels': '[46, 92, 184, 368]',
        'Depths': '[2, 2, 10, 2]',
        'Expected Memory': '~38 MB (float32)'
    }

    for key, value in reference.items():
        print(f"  {key:<20}: {value}")

    print("\nüí° SE HAI 340M PARAMETRI, POSSIBILI CAUSE:")
    print("  1. ‚ùå Dimensioni embedding/hidden troppo grandi")
    print("  2. ‚ùå Attention con troppi head o dimensioni sbagliate")
    print("  3. ‚ùå FFN con expansion ratio troppo alto (R=3.6 corretto)")
    print("  4. ‚ùå Layer duplicati o non condivisi")
    print("  5. ‚ùå Bug nel calcolo di d_k, d_v negli attention blocks")
    print("  6. ‚ùå Immagine troppo grande (1024 vs 224) causa explosion nei position embeddings")


def check_attention_params(model):
    """
    Controlla i parametri degli attention layers.
    """
    print("\nüîé ANALISI ATTENTION LAYERS:")
    print("-" * 80)

    attention_params = 0
    attention_count = 0

    for name, module in model.named_modules():
        # Cerca moduli che potrebbero essere attention
        if any(keyword in name.lower() for keyword in ['attn', 'attention', 'mhsa']):
            params = sum(p.numel() for p in module.parameters())
            if params > 0:
                attention_params += params
                attention_count += 1
                print(f"  {name:<50} {params:>12,} ({params / 1e6:.2f}M)")

    if attention_count > 0:
        print(f"\n  TOTALE ATTENTION: {attention_params:,} ({attention_params / 1e6:.2f}M)")
        print(f"  Numero di attention layers: {attention_count}")


# ============= SCRIPT PRINCIPALE =============

if __name__ == "__main__":
    print("\nüîß ISTRUZIONI:")
    print("=" * 80)
    print("1. Carica il tuo modello:")
    print("   from your_model import CMT_Ti")
    print("   model = CMT_Ti(img_size=1024, output_dim=256)")
    print()
    print("2. Esegui il debug:")
    print("   debug_model_parameters(model)")
    print("   find_problematic_layers(model)")
    print("   check_attention_params(model)")
    print()
    print("3. Manda l'output qui per analizzarlo!")
    print("=" * 80)

    print("\n\nüí° COSA CERCARE NELL'OUTPUT:")
    print("=" * 80)
    print("‚úì Stage 3 dovrebbe avere pi√π parametri (ha 10 blocks)")
    print("‚úì Ogni CMTBlock NON dovrebbe superare 1-2M parametri")
    print("‚úì Conv/Linear layers non dovrebbero essere troppo grandi")
    print("‚úì Position embeddings per 1024x1024 possono essere grandi!")
    print()
    print("‚ùå PROBLEMI COMUNI:")
    print("  ‚Ä¢ Position embedding: 1024*1024 = 1M pixels ‚Üí se embedded a 256D = 268M params!")
    print("  ‚Ä¢ Q,K,V projections con dimensioni sbagliate")
    print("  ‚Ä¢ FFN con hidden_dim = in_channels * R troppo grande")
    print("=" * 80)

    compare_with_reference()

    print("\n\nüö® SOSPETTO PRINCIPALE:")
    print("=" * 80)
    print("Se usi img_size=1024, il problema potrebbe essere nei position embeddings!")
    print()
    print("Calcolo:")
    print("  ‚Ä¢ Patches 1024x1024 con patch_size=4 ‚Üí 256x256 patches")
    print("  ‚Ä¢ Se ogni patch ha embedding 368D ‚Üí 256*256*368 = 24M params")
    print("  ‚Ä¢ Ma se sbagli e usi pixel-level embedding ‚Üí 1024*1024*368 = 386M params!")
    print()
    print("üí° SOLUZIONE: Controlla i position embeddings nel CMTBlock!")
    print("=" * 80)
